# -*- coding: utf-8 -*-
"""assignment5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a6tlXZNgaJbSrLv6gv6Ql5R6aYuxdFct
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install ipdb

import numpy as np
import pandas as pd
import ipdb
import pprint
import sys
import matplotlib.pyplot as plt
import math
import operator
import csv
from copy import deepcopy
from tabulate import tabulate

eps = np.finfo(float).eps
from numpy import log2 as log

def standardize_data(X):
    return (X - X.mean())/X.std()

root_path = "/content/gdrive/My Drive/Semester_#2/CSE471_SM_in_AI/Assignments/5/a5_2018201058"

data = pd.read_csv(root_path + "/input_data/Apparel/apparel-trainval.csv")

test_data = pd.read_csv(root_path + "/input_data/Apparel/apparel-test.csv")

# removing the output column
data_std = standardize_data(data.iloc[:, 1:])
# data_std = data.iloc[:, 1:]
data_std[["label"]] = data[["label"]]

# msk = np.random.rand(len(data)) < 0.8
# train = X_std[msk].reset_index (drop=True)
# validate = X_std[~msk].reset_index (drop=True)

# Selecting first 80% as Training Data and remaining as Validation Data
train, validate = np.split(data_std, [int(.8*len(data_std))])
validate = validate.reset_index(drop=True)

def one_hot_encoding_get(df):
    labels = list(range(10))
    labels.extend(list(df["label"]))

    s = pd.Series(labels)
    one_hot_encoding = pd.get_dummies(s).values
    return one_hot_encoding[10:, :]

validate_y = one_hot_encoding_get(validate)

def intercept_add(X):
    intercept = np.ones((X.shape[0], 1))
    return np.concatenate((intercept, X), axis=1)

def softmax_(x):
    """Compute the softmax of vector x."""
    # Removing the first column as it is all 1s
    x = x[:, 1:]
#     ipdb.set_trace()
    exps = np.exp(x)
    smax = exps / np.array([np.sum(exps, axis=1)]).T
#     return exps / np.array([np.sum(exps, axis=1)]).T
    return smax

# Activation Functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

def stable_softmax(x):
    """Compute the softmax of vector x in a numerically stable way."""
    shift_x = x - np.array([np.max(x, axis=1)]).T
    exps = np.exp(shift_x)
    return exps / np.array([np.sum(exps, axis=1)]).T

# Derivative Functions
def sigmoid_derivative(x):
    return x * (1 - x)

def relu_derivative(x):
    return 1. * (x > 0)

def tanh_derivative(x):
    return (1 - (x ** 2))

def cross_entropy_get(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions. 
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray        
    Returns: scalar
    """
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets * np.log(predictions + 1e-9))/N
    return ce

class NeuralNetwork:
    def __init__(self, num_neurons_list, actn_func = "sigmoid", lr = 0.1, wts = None):
        # initialize HyperParameters
        self.num_neurons_list = deepcopy(num_neurons_list)
        self.num_layers = len(self.num_neurons_list)
        for i in range(self.num_layers-1):
            self.num_neurons_list[i] += 1

        self.num_hidden_layers = self.num_layers - 2
        self.activation_func = actn_func
        self.lr = lr
        
        self.cross_entropies = []
        
        if wts is None:
            self.weights = []
            self.d_weights = []

            for i in range(self.num_layers-1):
                bound = np.sqrt(1./(self.num_neurons_list[i]))
                self.weights.append(np.random.uniform(-bound, bound, (self.num_neurons_list[i], self.num_neurons_list[i+1])))
                self.d_weights.append(np.zeros((self.num_neurons_list[i], self.num_neurons_list[i+1])))
        else:
            for i in range(len(wts)):
                if (self.num_neurons_list[i] != wts[i].shape[0]) or (self.num_neurons_list[i + 1] != wts[i].shape[1]):
                    print ("Weights provided have invalid dimensions!!")
                    sys.exit(-1)
                    
            self.weights = wts


    def forward_prop(self, X):
#         ipdb.set_trace()
        self.L_out = []

        X = intercept_add(X)
        self.L_out.append(X)
        prev_out = None

        for l in range(1, len(self.num_neurons_list)):
            prev_out = self.L_out[-1]
            curr_in = prev_out @ self.weights[l-1]
  
            if l == (len(self.num_neurons_list) - 1):
                curr_out = stable_softmax(curr_in)

            else:
                if self.activation_func == "sigmoid":
                    curr_out = sigmoid(curr_in)
                elif self.activation_func == "relu":
                    curr_out = relu(curr_in)
                elif self.activation_func == "tanh":
                    curr_out = tanh(curr_in)
                else:
                    print ("Invalid Activation Function!!")
                    sys.exit(-1)
                
            self.L_out.append(curr_out)



    def backward_prop(self, y):
#         ipdb.set_trace()
        self.d_weights[-1] = np.dot(self.L_out[-2].T, (2*(y - self.L_out[-1]) * sigmoid_derivative(self.L_out[-1])))

        for i in reversed(range(self.num_layers-2)):
            temp = None
            if self.activation_func == "sigmoid":
                  temp = sigmoid_derivative(self.L_out[i+1])
            elif self.activation_func == "relu":
                temp = relu_derivative(self.L_out[i+1])
            elif self.activation_func == "tanh":
                temp = tanh_derivative(self.L_out[i+1])
            else:
                print ("Invalid Activation Function!!")
                sys.exit(-1)

            self.d_weights[i] = np.dot(self.L_out[i].T,  (np.dot(2*(y - self.L_out[-1]) * sigmoid_derivative(self.L_out[-1]), self.weights[-1].T) * temp))

        for i in range(self.num_layers-1):
            self.weights[i] += (self.lr * self.d_weights[i])


    def fit(self, train_df, batch_size, num_epochs):
        n = len(train_df.index)
        train_y = one_hot_encoding_get(train_df)
        train_x = train_df.iloc[:, :-1].values
        for n_epoch in range(num_epochs):
            epoch_cost = 0.0
            s, e = 0, batch_size
            num_mini_bactches = 0
            while (s < n):
                num_mini_bactches += 1
                
                e = min(n, s + batch_size)

#                 ipdb.set_trace()
                self.forward_prop(train_x[s:e, :])
                self.backward_prop(train_y[s:e, :])

                minibatch_cost = cross_entropy_get(self.L_out[-1], train_y[s:e, :])
                epoch_cost += minibatch_cost

                s = e

            epoch_cost = epoch_cost / num_mini_bactches
            self.cross_entropies.append(epoch_cost)

            if n_epoch % 50 == 0:
                print ("No. of Epochs:", n_epoch, "CrossEntropy:", epoch_cost)


    def predictions_get(self, data_df):
        self.forward_prop(data_df.values)
        return self.L_out[-1].argmax(axis=1)

    def accuracy_get(self, validate_df):
        predictions = self.predictions_get(validate_df.iloc[:, :-1])
        accuracy = 100 * np.mean(predictions == validate_df["label"])
        return round(accuracy, 2)

def error_graph_plot(errors, num_epochs):
    fig, ax = plt.subplots(figsize=(12,6))
    ax.plot(list(range(num_epochs)), errors, color="red", lw=1, ls='-');
    plt.xlabel("Number of Epochs")
    plt.ylabel("Cross Entropy")
    plt.title("Error vs Number of Epochs")
    plt.show()

"""**Activation Function:** Sigmoid"""

NN_sigmoid = NeuralNetwork([784, 100, 10], "sigmoid", 0.001)
NN_sigmoid.fit(train, 500, 500)

sigm_1l_100_001_acc = NN_sigmoid.accuracy_get(validate)
print("Sigmoid Training accuracy:", sigm_1l_100_001_acc, "%")

sigm_1l_100_001_errors = NN_sigmoid.cross_entropies
error_graph_plot(sigm_1l_100_001_errors, 500)

NN_sigmoid2 = NeuralNetwork([784, 100, 100, 10], "sigmoid", 0.001)
NN_sigmoid2.fit(train, 500, 500)

sigm_2l_100_001_acc = NN_sigmoid2.accuracy_get(validate)
print("Sigmoid Training accuracy:", sigm_2l_100_001_acc, "%")

sigm_2l_100_001_errors = NN_sigmoid2.cross_entropies
error_graph_plot(sigm_2l_100_001_errors, len(sigm_2l_100_001_errors))

NN_sigmoid3 = NeuralNetwork([784, 100, 100, 100, 10], "sigmoid", 0.001)
NN_sigmoid3.fit(train, 500, 500)

sigm_3l_100_001_acc = NN_sigmoid3.accuracy_get(validate)
print("Sigmoid Training accuracy:", sigm_3l_100_001_acc, "%")

sigm_3l_100_001_errors = NN_sigmoid3.cross_entropies
error_graph_plot(sigm_3l_100_001_errors, len(sigm_3l_100_001_errors))

fig, ax = plt.subplots(figsize=(12,6))
ax.plot(list(range(len(sigm_1l_100_001_errors))), sigm_1l_100_001_errors, color="purple", lw=1, ls='-', label="1 Layer")
ax.plot(list(range(len(sigm_2l_100_001_errors))), sigm_2l_100_001_errors, color="green", lw=1, ls='-', label="2 Layers")
ax.plot(list(range(len(sigm_3l_100_001_errors))), sigm_3l_100_001_errors, color="yellow", lw=1, ls='-', label="3 Layers")
plt.xlabel("Number of Epochs")
plt.ylabel("Cross Entropy")
plt.title("Affect of Number of Layers on Prediction Loss ")
plt.legend()
plt.show()

NN_relu1 = NeuralNetwork([784, 100, 10], "relu", 0.001)
NN_relu1.fit(train, 500, 500)

relu_1l_100_001_acc = NN_relu1.accuracy_get(validate)
print("Sigmoid Training accuracy:", relu_1l_100_001_acc, "%")

relu_1l_100_001_errors = NN_relu1.cross_entropies
error_graph_plot(relu_1l_100_001_errors, len(relu_1l_100_001_errors))

NN_relu2 = NeuralNetwork([784, 100, 100, 10], "relu", 0.001)
NN_relu2.fit(train, 500, 500)

relu_2l_100_001_acc = NN_relu2.accuracy_get(validate)
print("Sigmoid Training accuracy:", relu_2l_100_001_acc, "%")

relu_2l_100_001_errors = NN_relu2.cross_entropies
error_graph_plot(relu_2l_100_001_errors, len(relu_2l_100_001_errors))

NN_relu3 = NeuralNetwork([784, 100, 100, 100, 10], "relu", 0.0001)
NN_relu3.fit(train, 500, 500)

# Use the best NN for this
best_predictions = NN_sigmoid.predictions_get(test_data)
best_predictions = [[p] for p in best_predictions]

with open(root_path + '/src/2018201058_prediction.csv', 'w') as csvFile:
    writer = csv.writer(csvFile)
    writer.writerows(best_predictions)

np.save(root_path + '/src/NN_sigmoid_weights', NN_sigmoid.weights)

loaded_wts = np.load(root_path + '/src/NN_sigmoid_weights.npy')
loaded_wts = loaded_wts.tolist()

loaded_NN_sigmoid = NeuralNetwork([784, 500, 10], "sigmoid", 0.00001, loaded_wts)
sigmoid_accuracy = loaded_NN_sigmoid.accuracy_get(validate)
print("Sigmoid Training accuracy:", sigmoid_accuracy, "%")

"""**Q2. House Price Prediction**

**Steps required to modify the above model for this task**

1. As can be seen the categorical features ‘Alley, ‘PoolQC’, and ‘MscFeature’ have columns missing. However, looking at the data description, the NA’s in categorical variables actually mean “not present”.  For example, NA in the ‘Alley’ variable meant that the home has no alley. To correct the issue, we could replaced the Na’s with “None”. or the remaining numerical features, NA’s could be replaced by the value zero.

2. Turn the categorical data into dummy columns. This involves creating a column for each of the category types within category columns and assigning a 1 or 0 to an entry if the category is met. 
     It could make the dataset become too complex to handle. If it is the case, complexity of the dataset can be reduced by dimensionality reduction tools, like principle component analysis (PCA) and independent component analysis (ICA).

3. Remove the original category columns.

4. As for numerical features, quite a few have a highly skewed distribution which can lead to poor models. To combat this, these features could be log transformed.
"""

