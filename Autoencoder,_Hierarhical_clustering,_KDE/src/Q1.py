# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GtDWC7hVxiLJLRkF3hsCBtCWrvoRaT8p
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install ipdb

import numpy as np
import pandas as pd
import ipdb
import pprint
import sys
import matplotlib.pyplot as plt
import math
import operator
import csv
from copy import deepcopy
from tabulate import tabulate

eps = np.finfo(float).eps
from numpy import log2 as log

def standardize_data(X):
    return (X - X.mean())/X.std()

root_path = "/content/gdrive/My Drive/Semester_#2/CSE471_SM_in_AI/Assignments/9/2018201058_assignment9"

data = pd.read_csv(root_path + "/input_data/data.csv")
labels = data.iloc[:, -1]

# removing the output column
data_std = standardize_data(data.iloc[:, :-1])
train = data_std

def intercept_add(X):
    intercept = np.ones((X.shape[0], 1))
    return np.concatenate((intercept, X), axis=1)

# Activation Functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

def linear(x, c):
    return c * x
  
def stable_softmax(x):
    """Compute the softmax of vector x in a numerically stable way."""
    shift_x = x - np.array([np.max(x, axis=1)]).T
    exps = np.exp(shift_x)
    return exps / np.array([np.sum(exps, axis=1)]).T

# Derivative Functions
def sigmoid_derivative(x):
    return x * (1 - x)

def relu_derivative(x):
    return 1. * (x > 0)

def tanh_derivative(x):
    return (1 - (x ** 2))

def linear_derivative(x, c):
    return np.full((x.shape[0], x.shape[1]), c)

def mse(X, Y):
    errors = X - Y
    square_errors = np.power(errors, 2)
    return errors, square_errors, (np.sum(square_errors) / (2 * len(X)))

def cross_entropy_get(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions. 
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray        
    Returns: scalar
    """
#     predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets * np.log(predictions + 1e-9))/N
    return ce

class NeuralNetwork:
    def __init__(self, num_neurons_list, actn_func = "sigmoid", lr = 0.1, wts = None):
        # initialize HyperParameters
        self.num_neurons_list = deepcopy(num_neurons_list)
        self.num_layers = len(self.num_neurons_list)
        for i in range(self.num_layers-1):
            self.num_neurons_list[i] += 1

        self.num_hidden_layers = self.num_layers - 2
        self.activation_func = actn_func
        self.lr = lr

        self.cross_entropies = []

        if wts is None:
            self.weights = []
            self.d_weights = []

            for i in range(self.num_layers-1):
                bound = np.sqrt(1./(self.num_neurons_list[i]))
                self.weights.append(np.random.uniform(-bound, bound, (self.num_neurons_list[i], self.num_neurons_list[i+1])))
                self.d_weights.append(np.zeros((self.num_neurons_list[i], self.num_neurons_list[i+1])))
        else:
            for i in range(len(wts)):
                if (self.num_neurons_list[i] != wts[i].shape[0]) or (self.num_neurons_list[i + 1] != wts[i].shape[1]):
                    print ("Weights provided have invalid dimensions!!")
                    sys.exit(-1)

            self.weights = wts


    def forward_prop(self, X):
        self.L_out = []
        self.L_in = []

        X = intercept_add(X)
        self.L_in.append(X)
        self.L_out.append(X)
        prev_out = None

        for l in range(1, len(self.num_neurons_list)):
            prev_out = self.L_out[-1]
            curr_in = prev_out @ self.weights[l-1]
            self.L_in.append(curr_in)

            if l == (len(self.num_neurons_list)//2):
                curr_out = sigmoid(curr_in)
            else:
                if self.activation_func == "sigmoid":
                    curr_out = sigmoid(curr_in)
                elif self.activation_func == "relu":
                    curr_out = relu(curr_in)
                elif self.activation_func == "tanh":
                    curr_out = tanh(curr_in)
                elif self.activation_func == "linear":
                    curr_out = linear(curr_in, 1)
                else:
                    print ("Invalid Activation Function!!")
                    sys.exit(-1)
            self.L_out.append(curr_out)


    def bottleneck_output_get(self, X):
        X = intercept_add(X)
        curr_out = X
        prev_out = None

        for l in range(1, (len(self.num_neurons_list) // 2) + 1):
            prev_out = curr_out
            curr_in = prev_out @ self.weights[l-1]

            if l == (len(self.num_neurons_list)//2):
                curr_out = sigmoid(curr_in)
            else:
                if self.activation_func == "sigmoid":
                    curr_out = sigmoid(curr_in)
                elif self.activation_func == "relu":
                    curr_out = relu(curr_in)
                elif self.activation_func == "tanh":
                    curr_out = tanh(curr_in)
                elif self.activation_func == "linear":
                    curr_out = linear(curr_in, 1)
                else:
                    print ("Invalid Activation Function!!")
                    sys.exit(-1)

        return curr_out[:, 1:]


    def backward_prop(self, y):
        error = 2 * (y - self.L_out[-1])
        
        for i in reversed(range(self.num_layers-1)):
            if i == ((self.num_layers // 2) - 1) or self.activation_func == "sigmoid":
                self.d_weights[i] = np.dot(self.L_out[i].T, (error * sigmoid_derivative(self.L_in[i+1])))
            elif self.activation_func == "relu":
                self.d_weights[i] = np.dot(self.L_out[i].T, (error * relu_derivative(self.L_in[i+1])))
            elif self.activation_func == "tanh":
                self.d_weights[i] = np.dot(self.L_out[i].T, (error * tanh_derivative(self.L_in[i+1])))
            elif self.activation_func == "linear":
                self.d_weights[i] = np.dot(self.L_out[i].T, (error * linear_derivative(self.L_in[i+1], 1)))
            else:
                print ("Invalid Activation Function!!")
                sys.exit(-1)

            error = error @ self.weights[i].T

        for i in range(self.num_layers-1):
            self.weights[i] += (self.lr * self.d_weights[i])


    def fit(self, train_df, batch_size, num_epochs):
        n = len(train_df.index)
        train_x = train_df.iloc[:, :].values
        for n_epoch in range(num_epochs):
            epoch_cost = 0.0
            s, e = 0, batch_size
            num_mini_bactches = 0
            while (s < n):
                num_mini_bactches += 1

                e = min(n, s + batch_size)

                self.forward_prop(train_x[s:e, :]/100)
                self.backward_prop(train_x[s:e, :]/100)

                minibatch_cost = mse(self.L_out[-1], train_x[s:e, :]/100)
                epoch_cost += minibatch_cost[2]

                s = e

            epoch_cost = epoch_cost / num_mini_bactches
            self.cross_entropies.append(epoch_cost)

            if n_epoch % 50 == 0:
                print ("No. of Epochs:", n_epoch, "MSE:", epoch_cost)


    def predictions_get(self, data_df):
        self.forward_prop(data_df.values)
        return self.L_out[-1].argmax(axis=1)

    def accuracy_get(self, validate_df):
        predictions = self.predictions_get(validate_df.iloc[:, :-1])
        accuracy = 100 * np.mean(predictions == validate_df["label"])
        return round(accuracy, 2)

# def error_graph_plot(errors, num_epochs):
#     fig, ax = plt.subplots(figsize=(12,6))
#     ax.plot(list(range(num_epochs)), errors, color="red", lw=1, ls='-');
#     plt.xlabel("Number of Epochs")
#     plt.ylabel("Cross Entropy")
#     plt.title("Error vs Number of Epochs")
#     plt.show()

"""## Question 1

### Part-1-a: Sparce Autoencoder with Linear activation functions
"""

sparce_autoencoder_linear = NeuralNetwork([29, 14, 29], "linear", 0.0001)
sparce_autoencoder_linear.fit(train, 500, 1000)
reduced_X1 = sparce_autoencoder_linear.bottleneck_output_get(train)

reduced_X1.shape

"""### Part-1-b: Sparce Autoencoder with Non-Linear activation functions"""

sparce_autoencoder_relu = NeuralNetwork([29, 14, 29], "relu", 0.00001)
sparce_autoencoder_relu.fit(train, 500, 500)
reduced_X2 = sparce_autoencoder_relu.bottleneck_output_get(train)

reduced_X2.shape

"""### Part-1-c: Deep Autoencoder with Non-Linear activation functions"""

deep_autoencoder_sigmoid = NeuralNetwork([29, 20, 14, 20, 29], "sigmoid", 0.00000001)
deep_autoencoder_sigmoid.fit(train, 500, 1000)
reduced_X3 = deep_autoencoder_sigmoid.bottleneck_output_get(train)

reduced_X3.shape

"""## Part-2: K-means"""

def KMeans(data, k, max_iterations):
    n = data.shape[0]
    c = data.shape[1]
    
    std = np.std(data, axis = 0)
    mean = np.mean(data, axis = 0)
    centers = np.random.randn(k,c)*std + mean

    centers_old = np.zeros(centers.shape)   # to store old centers
    centers_new = deepcopy(centers)         # Store new centers

    clusters = np.zeros(n)
    distances = np.zeros((n,k))
    error = np.linalg.norm(centers_new - centers_old)
    
    fig, ax = plt.subplots(figsize=(16,8))
    ax.scatter(data[:,0], data[:,1], s=7)
    ax.scatter(centers_new[:,0], centers_new[:,1], marker='*', c='r', s=150)
    plt.show()

    # When, after an update, the estimate of that center stays the same, exit loop
    while max_iterations != 0 and error != 0:        
        # Measure the distance to every center
        for i in range(k):
            distances[:,i] = np.linalg.norm(data - centers_new[i], axis=1)

        # Assign all training data to closest center
        clusters = np.argmin(distances, axis = 1)

        centers_old = deepcopy(centers_new)
        # Calculate mean for every cluster and update the center
        for i in range(k):
            centers_new[i] = np.mean(data[clusters == i], axis=0)

        error = np.linalg.norm(centers_new - centers_old)
        max_iterations -= 1

    print (max_iterations)
    fig, ax = plt.subplots(figsize=(14,7))
    colors = ['r', 'g', 'b', 'c', 'y']
    markers = ['+', '^', 'o', 'v', 's']
    for i in range(k):
        ax.scatter(data[clusters == i][:, 0], data[clusters == i][:, 1], s=7, marker=markers[i], c=colors[i])
        ax.scatter(centers_new[i,0], centers_new[i,1], marker='*', c='black', s=150)
    plt.show()

    return clusters, centers_new

"""### reduced_X1 (K-Means)"""

clusters, centers = KMeans(reduced_X1, 5, 100)

cluster_series = pd.Series(clusters, name='Clusters')
label_series = pd.Series(labels, name='Classes')
df_confusion = pd.crosstab(label_series, cluster_series, margins=True)
df_confusion = df_confusion.iloc[:-1, :-1]
print(tabulate(df_confusion, headers='keys', tablefmt='psql'))

# Pie chart using Pandas
for i in range(0,5):
    plot = df_confusion.plot.pie(y= i, figsize=(5, 5))

def purity_get(confusion_df):
    max_vals = np.array([confusion_df.max()])
    purity = max_vals.sum()/data.shape[0]
    return purity

purity_X1_kmeans =  purity_get(df_confusion)
purity_X1_kmeans

"""### reduced_X2 (K-Means)"""

clusters, centers = KMeans(reduced_X2, 5, 100)

cluster_series = pd.Series(clusters, name='Clusters')
label_series = pd.Series(labels, name='Classes')
df_confusion = pd.crosstab(label_series, cluster_series, margins=True)
df_confusion = df_confusion.iloc[:-1, :-1]
print(tabulate(df_confusion, headers='keys', tablefmt='psql'))

# Pie chart using Pandas
for i in range(0,5):
    plot = df_confusion.plot.pie(y= i, figsize=(5, 5))

purity_X2_kmeans =  purity_get(df_confusion)
purity_X2_kmeans

"""### reduced_X3 (K-Means)"""

clusters, centers = KMeans(reduced_X3, 5, 100)

cluster_series = pd.Series(clusters, name='Clusters')
label_series = pd.Series(labels, name='Classes')
df_confusion = pd.crosstab(label_series, cluster_series, margins=True)
df_confusion = df_confusion.iloc[:-1, :-1]
print(tabulate(df_confusion, headers='keys', tablefmt='psql'))

purity_X3_kmeans =  purity_get(df_confusion)
purity_X3_kmeans

"""## Part-3: GMM

### Imports
"""

from sklearn.mixture import GaussianMixture

"""### reduced_X1 (GMM)"""

gmm = GaussianMixture(n_components=5, random_state=3)
clf = gmm.fit(reduced_X1)
clf.weights_

fig, ax = plt.subplots(figsize=(14,7))
ax.scatter(reduced_X1[:,0], reduced_X1[:,1])
for i in range(5):
    ax.scatter(gmm.means_[i,0], gmm.means_[i,1], marker='*', c='black', s=150)
plt.show()

pred_clusters = gmm.predict(reduced_X1)
pred_clusters

cluster_series = pd.Series(pred_clusters, name='Clusters')
label_series = pd.Series(labels, name='Classes')
gmm_confusion_df = pd.crosstab(label_series, cluster_series, margins=True)
gmm_confusion_df = gmm_confusion_df.iloc[:-1, :-1]
print(tabulate(gmm_confusion_df, headers='keys', tablefmt='psql'))

for i in range(0,5):
    plot = gmm_confusion_df.plot.pie(y= i, figsize=(5, 5))

purity_X1_gmm =  purity_get(gmm_confusion_df)
purity_X1_gmm

"""### reduced_X2 (GMM)"""

clf = gmm.fit(reduced_X2)
clf.weights_

fig, ax = plt.subplots(figsize=(14,7))
ax.scatter(reduced_X2[:,0], reduced_X2[:,1])
for i in range(5):
    ax.scatter(gmm.means_[i,0], gmm.means_[i,1], marker='*', c='black', s=150)
plt.show()

pred_clusters = gmm.predict(reduced_X2)
pred_clusters

cluster_series = pd.Series(pred_clusters, name='Clusters')
label_series = pd.Series(labels, name='Classes')
gmm_confusion_df = pd.crosstab(label_series, cluster_series, margins=True)
gmm_confusion_df = gmm_confusion_df.iloc[:-1, :-1]
print(tabulate(gmm_confusion_df, headers='keys', tablefmt='psql'))

for i in range(0,5):
    plot = gmm_confusion_df.plot.pie(y= i, figsize=(5, 5))

purity_X2_gmm =  purity_get(gmm_confusion_df)
purity_X2_gmm

"""### reduced_X3 (GMM)"""

clf = gmm.fit(reduced_X3)
clf.weights_

fig, ax = plt.subplots(figsize=(14,7))
ax.scatter(reduced_X3[:,0], reduced_X3[:,1])
for i in range(5):
    ax.scatter(gmm.means_[i,0], gmm.means_[i,1], marker='*', c='black', s=150)
# ax.axis('equal')
plt.show()

pred_clusters = gmm.predict(reduced_X3)
pred_clusters

cluster_series = pd.Series(pred_clusters, name='Clusters')
label_series = pd.Series(labels, name='Classes')
gmm_confusion_df = pd.crosstab(label_series, cluster_series, margins=True)
gmm_confusion_df = gmm_confusion_df.iloc[:-1, :-1]
print(tabulate(gmm_confusion_df, headers='keys', tablefmt='psql'))

for i in range(0,5):
    plot = gmm_confusion_df.plot.pie(y= i, figsize=(5, 5))

purity_X3_gmm =  purity_get(gmm_confusion_df)
purity_X3_gmm

"""## Part-4: Hierarchical Clustering"""

from sklearn.cluster import AgglomerativeClustering

"""### reduced_X1"""

cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  
cluster.fit_predict(reduced_X1)

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(reduced_X1[:,0], reduced_X1[:,1], c=cluster.labels_, cmap='rainbow')

cluster_series = pd.Series(cluster.labels_, name='Clusters')
label_series = pd.Series(labels, name='Classes')
hc_confusion_df = pd.crosstab(label_series, cluster_series, margins=True)
hc_confusion_df = hc_confusion_df.iloc[:-1, :-1]
print(tabulate(hc_confusion_df, headers='keys', tablefmt='psql'))

for i in range(0,5):
    plot = hc_confusion_df.plot.pie(y= i, figsize=(5, 5))

purity_X1_hc =  purity_get(hc_confusion_df)
purity_X1_hc

"""### reduced_X2 (Hierarchical Clustering)"""

cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  
cluster.fit_predict(reduced_X2)

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(reduced_X2[:,0], reduced_X2[:,1], c=cluster.labels_, cmap='rainbow')

cluster_series = pd.Series(cluster.labels_, name='Clusters')
label_series = pd.Series(labels, name='Classes')
hc_confusion_df = pd.crosstab(label_series, cluster_series, margins=True)
hc_confusion_df = hc_confusion_df.iloc[:-1, :-1]
print(tabulate(hc_confusion_df, headers='keys', tablefmt='psql'))

for i in range(0,5):
    plot = hc_confusion_df.plot.pie(y= i, figsize=(5, 5))

purity_X2_hc =  purity_get(hc_confusion_df)
purity_X2_hc

"""### reduced_X3 (Hierarchical Clustering)"""

cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  
cluster.fit_predict(reduced_X3)

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(reduced_X3[:,0], reduced_X3[:,1], c=cluster.labels_, cmap='rainbow')

cluster_series = pd.Series(cluster.labels_, name='Clusters')
label_series = pd.Series(labels, name='Classes')
hc_confusion_df = pd.crosstab(label_series, cluster_series, margins=True)
hc_confusion_df = hc_confusion_df.iloc[:-1, :-1]
print(tabulate(hc_confusion_df, headers='keys', tablefmt='psql'))

for i in range(0,5):
    plot = hc_confusion_df.plot.pie(y= i, figsize=(5, 5))

purity_X3_hc =  purity_get(hc_confusion_df)
purity_X3_hc

"""## Part-5: Create a pie chart comparing purity of different clustering methods you have tried for all classes for the different autoencoders."""

def draw_purity_pie_chart(purity1, purity2, purity3):
    labels_Part_a = 'KMeans', 'GMM', 'Hierarchical'
    sizes = [purity1,purity2,purity3]
    fig1, ax1 = plt.subplots()
    ax1.pie(sizes, labels=labels_Part_a, autopct='%1.1f%%',shadow=True, startangle=90)
    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

"""### reduced_X1 data"""

draw_purity_pie_chart(purity_X1_kmeans, purity_X1_gmm, purity_X1_hc)

"""### reduced_X2"""

draw_purity_pie_chart(purity_X2_kmeans, purity_X2_gmm, purity_X2_hc)

"""### reduced_X3"""

draw_purity_pie_chart(purity_X3_kmeans, purity_X3_gmm, purity_X3_hc)